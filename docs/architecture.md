# ğŸ—ï¸ Architecture Technique du Chatbot IA GÃ©nÃ©ratif

## ğŸ§  1. Objectif du Projet

Ce projet a pour but de concevoir un chatbot intelligent capable de rÃ©pondre Ã  des questions en langage naturel, tout en sâ€™appuyant sur des documents techniques internes (via le RAG - Retrieval-Augmented Generation). Lâ€™IA peut donc combiner ses connaissances gÃ©nÃ©rales avec des donnÃ©es spÃ©cifiques fournies par lâ€™utilisateur.

---

## ğŸ—ï¸ 2. Architecture GÃ©nÃ©rale

L'architecture est basÃ©e sur une structure **Full Stack** :

- **Frontend** : Next.js (React)
- **Backend** : FastAPI avec Uvicorn
- **Moteur IA** : LangChain + HuggingFace Embeddings
- **Base vectorielle** : Pinecone
- **Stockage local des fichiers** : `data/docs/`

---

## âš™ï¸ 3. Pipeline de Traitement (RAG)

### ğŸ“¥ Ã‰tape 1 : Ingestion des documents
- Formats supportÃ©s : `.pdf`, `.docx`, `.txt`, `.md`, `.html`
- Utilisation de LangChain (`PyPDFLoader`, `TextLoader`, etc.)
- Les documents sont stockÃ©s dans `data/docs/`.

### âœ‚ï¸ Ã‰tape 2 : DÃ©coupage (chunking)
- DÃ©coupage des documents en morceaux de 1000 caractÃ¨res avec chevauchement de 200 (`RecursiveCharacterTextSplitter`)

### ğŸ§  Ã‰tape 3 : Embedding
- ModÃ¨le utilisÃ© : `sentence-transformers/all-MiniLM-L6-v2` via `HuggingFaceEmbeddings`
- Chaque chunk est transformÃ© en vecteur.

### ğŸ—ƒï¸ Ã‰tape 4 : Indexation dans Pinecone
- Pinecone est utilisÃ© comme base de donnÃ©es vectorielle.
- Index dynamique crÃ©Ã© si non existant.

### â“ Ã‰tape 5 : Question/RÃ©ponse
- Lâ€™utilisateur pose une question via lâ€™interface.
- Le backend utilise `retriever.invoke()` pour retrouver les chunks les plus pertinents.
- Une rÃ©ponse est gÃ©nÃ©rÃ©e par lâ€™IA Ã  partir de la question + documents retrouvÃ©s.

---

## ğŸ–¥ï¸ 4. Frontend (Next.js)

- Interface utilisateur avec React
- Composants : `ChatBox`, `ChatBubble`, `ChatToolbar`, `ChatInput`, `TypingIndicator`
- Historique des conversations sauvegardÃ© dans `localStorage`
- Affichage des sources si disponibles

---

## ğŸ§ª 5. Backend (FastAPI)

- Route principale : `POST /chat`
- Analyse de la requÃªte utilisateur, rÃ©cupÃ©ration des documents, gÃ©nÃ©ration de rÃ©ponse
- Appels aux modules LangChain, embeddings, Pinecone

---

## ğŸ” 6. SÃ©curitÃ© & ConfidentialitÃ©

- Aucune donnÃ©e sensible nâ€™est stockÃ©e sur un serveur
- Les documents sont locaux ou hÃ©bergÃ©s temporairement pour indexation
- Pinecone est configurÃ© avec des clÃ©s API sÃ©curisÃ©es via `.env`

---

## ğŸ“‚ 7. RÃ©pertoires ClÃ©s

chatbot-project/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ rag_pipeline.py # pipeline RAG principal
â”‚ â”œâ”€â”€ server.py # API FastAPI
â”‚ â”œâ”€â”€ ingest_documents.py # ingestion des documents
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ src/components/ # Chat UI
â”‚
â”œâ”€â”€ data/docs/ # documents Ã  ingÃ©rer (PDF, Word, etc.)
â”œâ”€â”€ .env # clÃ©s API Pinecone, etc.
â””â”€â”€ docs/architecture.md # cette documentation technique
â””â”€â”€ docs/guid_utilisateur.md


---

## ğŸ 8. Limites et AmÃ©liorations Futures

- Lâ€™IA ne connaÃ®t que les documents fournis (pas dâ€™accÃ¨s internet).
- PossibilitÃ© dâ€™ajouter lâ€™upload dynamique de fichiers par lâ€™utilisateur.
- IntÃ©gration future possible de modÃ¨les plus puissants (OpenAI, Mistral, Claude...).

## 9. SchÃ©ma 
Utilisateur (Next.js)
       |
       v
Question â†’ Frontend â†’ API FastAPI (/chat)
                         |
                         v
       [RAG Pipeline - LangChain]
            â”œâ”€â”€ Retriever (Pinecone)
            â”œâ”€â”€ LLM (ChatOpenAI / HuggingFace)
            â””â”€â”€ GÃ©nÃ©ration rÃ©ponse
                         |
                         v
                 RÃ©ponse + Sources
                         |
                         v
                   Frontend (UI)

